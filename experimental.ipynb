{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experimental.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNjPbAYmwJp8y/Ep+dl/Ns3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reggy0/YOLO5/blob/main/experimental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YI2PVwA67hon"
      },
      "outputs": [],
      "source": [
        "# This file contains experimental modules\n",
        "\n",
        "from models.common import *\n",
        "\n",
        "\n",
        "class CrossConv(nn.Module):\n",
        "    # Cross Convolution Downsample\n",
        "    def __init__(self, c1, c2, k=3, s=1, g=1, e=1.0, shortcut=False):\n",
        "        # ch_in, ch_out, kernel, stride, groups, expansion, shortcut\n",
        "        super(CrossConv, self).__init__()\n",
        "        c_ = int(c2 * e)  # hidden channels\n",
        "        self.cv1 = Conv(c1, c_, (1, k), (1, s))\n",
        "        self.cv2 = Conv(c_, c2, (k, 1), (s, 1), g=g)\n",
        "        self.add = shortcut and c1 == c2\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
        "\n",
        "\n",
        "class C3(nn.Module):\n",
        "    # Cross Convolution CSP\n",
        "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n",
        "        super(C3, self).__init__()\n",
        "        c_ = int(c2 * e)  # hidden channels\n",
        "        self.cv1 = Conv(c1, c_, 1, 1)\n",
        "        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n",
        "        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n",
        "        self.cv4 = Conv(2 * c_, c2, 1, 1)\n",
        "        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\n",
        "        self.act = nn.LeakyReLU(0.1, inplace=True)\n",
        "        self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        y1 = self.cv3(self.m(self.cv1(x)))\n",
        "        y2 = self.cv2(x)\n",
        "        return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))\n",
        "\n",
        "\n",
        "class Sum(nn.Module):\n",
        "    # Weighted sum of 2 or more layers https://arxiv.org/abs/1911.09070\n",
        "    def __init__(self, n, weight=False):  # n: number of inputs\n",
        "        super(Sum, self).__init__()\n",
        "        self.weight = weight  # apply weights boolean\n",
        "        self.iter = range(n - 1)  # iter object\n",
        "        if weight:\n",
        "            self.w = nn.Parameter(-torch.arange(1., n) / 2, requires_grad=True)  # layer weights\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x[0]  # no weight\n",
        "        if self.weight:\n",
        "            w = torch.sigmoid(self.w) * 2\n",
        "            for i in self.iter:\n",
        "                y = y + x[i + 1] * w[i]\n",
        "        else:\n",
        "            for i in self.iter:\n",
        "                y = y + x[i + 1]\n",
        "        return y\n",
        "\n",
        "\n",
        "class GhostConv(nn.Module):\n",
        "    # Ghost Convolution https://github.com/huawei-noah/ghostnet\n",
        "    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):  # ch_in, ch_out, kernel, stride, groups\n",
        "        super(GhostConv, self).__init__()\n",
        "        c_ = c2 // 2  # hidden channels\n",
        "        self.cv1 = Conv(c1, c_, k, s, g, act)\n",
        "        self.cv2 = Conv(c_, c_, 5, 1, c_, act)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.cv1(x)\n",
        "        return torch.cat([y, self.cv2(y)], 1)\n",
        "\n",
        "\n",
        "class GhostBottleneck(nn.Module):\n",
        "    # Ghost Bottleneck https://github.com/huawei-noah/ghostnet\n",
        "    def __init__(self, c1, c2, k, s):\n",
        "        super(GhostBottleneck, self).__init__()\n",
        "        c_ = c2 // 2\n",
        "        self.conv = nn.Sequential(GhostConv(c1, c_, 1, 1),  # pw\n",
        "                                  DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw\n",
        "                                  GhostConv(c_, c2, 1, 1, act=False))  # pw-linear\n",
        "        self.shortcut = nn.Sequential(DWConv(c1, c1, k, s, act=False),\n",
        "                                      Conv(c1, c2, 1, 1, act=False)) if s == 2 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x) + self.shortcut(x)\n",
        "\n",
        "\n",
        "class MixConv2d(nn.Module):\n",
        "    # Mixed Depthwise Conv https://arxiv.org/abs/1907.09595\n",
        "    def __init__(self, c1, c2, k=(1, 3), s=1, equal_ch=True):\n",
        "        super(MixConv2d, self).__init__()\n",
        "        groups = len(k)\n",
        "        if equal_ch:  # equal c_ per group\n",
        "            i = torch.linspace(0, groups - 1E-6, c2).floor()  # c2 indices\n",
        "            c_ = [(i == g).sum() for g in range(groups)]  # intermediate channels\n",
        "        else:  # equal weight.numel() per group\n",
        "            b = [c2] + [0] * groups\n",
        "            a = np.eye(groups + 1, groups, k=-1)\n",
        "            a -= np.roll(a, 1, axis=1)\n",
        "            a *= np.array(k) ** 2\n",
        "            a[0] = 1\n",
        "            c_ = np.linalg.lstsq(a, b, rcond=None)[0].round()  # solve for equal weight indices, ax = b\n",
        "\n",
        "        self.m = nn.ModuleList([nn.Conv2d(c1, int(c_[g]), k[g], s, k[g] // 2, bias=False) for g in range(groups)])\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = nn.LeakyReLU(0.1, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.act(self.bn(torch.cat([m(x) for m in self.m], 1)))"
      ]
    }
  ]
}